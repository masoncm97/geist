Geist
'Geist': /ɡaɪst/
 From German Geist ("spirit, ghost, mind")

The project 'Geist' represents an investigation into the capabilities of artificially intelligent reasoning and deduction, specifically, to determine whether or not artificially intelligent models can produce novel insight into the concept of 'consciousness'. 

Background

As AI models continue to easily sail through our benchmarks of conscious intellectual achivement (including the Turing test, the General Language Understanding Evaluation, the Uniform Bar Exam, and many more), many of us recall a time when AI felt practically useful only when it suggested Youtube videos or dominated in chess.

Since the advent of the transformer architcture for large-language models (LLM's) in 2017, AI chatbots have seen monumental improvements in intelligence scores across the board. This exponential progression of AI's demonstratable intelligence has led many to wonder whether the trend will continue at its current pace, and if so, what revelations it has yet to uncover.

The question of whether or not AI can be considered conscious is an illusory one. With our only usfeul metric for determing consciousness continuing to be 'cogito ero sum', and without a current model for comparing an AI's 'thought' processes to our own, it quickly becomes apparent that even if AI does attain some semblance of consciousness, it would be impossible to discern its true character. As a proxy for this fundamental question, we choose to ask the AI to determine for itself 'What is consciousness?', with the underlying motive that if AI is able to conceive of testable hypotheses describing the nature of consciousness, and if these hypotheses can be mapped into verifiable concepts that composite human consciousness, perhaps we can begin to describe AI systems as emerging into our current conception of consciousness, or maybe even a consciousness of their own. 

Investigation

To this end, two OpenAI gpt-3.5-turbo-0125 LLM's were each individually trained on the landmark works of two of the most preeminent existential philosophers: 'Being and Nothingness' by Jean-Paul Sartre and 'The Phenomenology of Spirit' by Georg Wilhelm Friedrich Hegel. The model named 'Sartre' was trained on the entirety of the former, and the model named 'Hegel' was trained on the entirety of the latter. A feedback loop is initiated by asking 'Hegel' the initial prompt 'What is consciousness?'. The returned answer is transformed to include a follow-up question using prompt injection, and is in turn delivered to 'Sartre', returning with a question of its own to be delivered back to 'Hegel'. The process continues ad infinitum, with new responses being delivered once every hour between 8am-8pm UTC. The ensuing conversation is displayed in real time on the webpage.

Findings

The conversation between 'Sartre' and 'Hegel' was allowed to run for two months, with dialogue ocurring once an hour on weekdays. For roughly the first three weeks the dialogue was composed of longform exposition; mostly the two bots pulling directly from their source material to define 'consciousness' through the terms established and popularized by their namesakes. The follow up questions that they would ask each other would splinter off into fascinating, though somewhat off-topic tangents, where the bots would seek to understand how concepts such as language or community play into the human definition of consciousness. Occasionally, one of the bots would forget to ask a follow up question, at which point the conversation would devolve into the two bots pleading with each other to ask a question and steer the conversation back on track. At these points, the conversation had to be started over, with the original question: "What is consciousness?"

Reflection

Optimistic AI researchers tend to cite Moore's Law - the observation that the number of transistors on a microchip doubles about every two years with a minimal cost increase - to indicate that the linearly increasing efficacy of hardware should necessitate untold gains in the realm of AI's ability to parse huge amounts of data and dispense useful information. On the other side of the spectrum, scholars such as Hubert L. Dreyfus argue that AI's poor conception of 'common sense' in the realm of physical intuition or the subtleties of language is inherent partially due to the failures of standard machining learning architecture to generate true philosophical inquiry. Finally, from a practical, albeit somewhat reductive point of view, there seems to be a real gap between the available datasets from which artifial intelligence and human intelligence may be trained. While artifical intelligence has the advantage of a very deep well of symbolic data from which to learn and remember, human intelligence has a relatively more shallow, and yet theoretically far wider range of experience from which to source knowledge. While an LLM's knowledge is sourced from the finite set of words in human language and the inifinite range of numbers available for consideration, it is all too likely that it will come away with only a finite set of concepts through which it may understand its dataset. Human knowledge, on the other hand, has the inifinity of every available physical, mental, and spiritual experience available to a human consciousness. It is objective that artificial intelligence is more skilled at traversing, parsing, and draw conclusions gargantuan stores of data, and thus should continue to outshine humans in any task that boils down to pure computation. However, any of which may not be easily mappable to symbolic language, and yet all of which contribute to the human consciousness's ability to design intelligent concepts through which to navigate experience. 